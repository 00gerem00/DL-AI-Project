{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQzv71vPNjWmXJMyFm3lD5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import shutil\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","path_MG = '/content/drive/MyDrive/00gerem00/DL-AI-Project'\n","path_LM = '/content/drive/MyDrive/lorenzomeroni02/DL-AI-Project'\n","\n","if os.path.exists(path_MG):\n","    working_dir = path_MG\n","    print(f\"Working in MG folder: {working_dir}\")\n","elif os.path.exists(path_LM):\n","    working_dir = path_LM\n","    print(f\"Working in LM folder: {working_dir}\")\n","else:\n","    print(\"Error: Project folder not found in Drive. Check the paths.\")\n","    working_dir = None\n","\n","if working_dir:\n","    os.chdir(working_dir)\n","    zip_filename = 'leather-defect-classification.zip'\n","    fast_local_dir = '/content/fast_dataset'\n","\n","    if os.path.exists(zip_filename):\n","        print(f\"Great! {zip_filename} found on Drive.\")\n","\n","        if not os.path.exists(fast_local_dir):\n","            os.makedirs(fast_local_dir, exist_ok=True)\n","            !unzip -q {zip_filename} -d {fast_local_dir}\n","    else:\n","        print(f\"Error: {zip_filename} not found in your Drive folder. Please check if you uploaded it correctly to {working_dir}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfKoDbe_WtZ8","executionInfo":{"status":"ok","timestamp":1772099948159,"user_tz":-60,"elapsed":2068,"user":{"displayName":"Matteo Gerevini","userId":"01604553129882149651"}},"outputId":"086d281e-526c-437f-a0bb-a9b932605ff0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Working in MG folder: /content/drive/MyDrive/00gerem00/DL-AI-Project\n","Great! leather-defect-classification.zip found on Drive.\n"]}]},{"cell_type":"code","source":["from torchvision.datasets import ImageFolder\n","\n","base_dir = '/content/fast_dataset'\n","\n","content = os.listdir(base_dir)\n","subfolders = [f for f in content if os.path.isdir(os.path.join(base_dir, f))][0]\n","data_dir = os.path.join(base_dir, subfolders)\n","\n","dataset = ImageFolder(\n","    root = data_dir,\n","    transform = None)\n","\n","print(f\"Total number of images: {len(dataset)}\")\n","print(f\"Classes found: {dataset.classes}\")\n","print(f\"Index mapping: {dataset.class_to_idx}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjN-EuzGY2LW","executionInfo":{"status":"ok","timestamp":1772099989304,"user_tz":-60,"elapsed":10859,"user":{"displayName":"Matteo Gerevini","userId":"01604553129882149651"}},"outputId":"a7ab4e92-9dac-4b2b-8bd8-d4e39ce1ac1b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of images: 3600\n","Classes found: ['Folding marks', 'Grain off', 'Growth marks', 'loose grains', 'non defective', 'pinhole']\n","Index mapping: {'Folding marks': 0, 'Grain off': 1, 'Growth marks': 2, 'loose grains': 3, 'non defective': 4, 'pinhole': 5}\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","\n","dataset.samples = [s for s in dataset.samples if Image.open(s[0]).size == (227, 227)]\n","dataset.imgs = dataset.samples\n","dataset.targets = [s[1] for s in dataset.samples]\n","print(f\"{len(dataset)} images\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2ui7GoTZEAk","executionInfo":{"status":"ok","timestamp":1772100013720,"user_tz":-60,"elapsed":383,"user":{"displayName":"Matteo Gerevini","userId":"01604553129882149651"}},"outputId":"d88e1da8-6a72-4690-82c4-cec8721333f4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["3598 images\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import random_split\n","\n","train_size = int(0.7 * len(dataset))\n","val_size = (len(dataset) - train_size) // 2\n","test_size = len(dataset) - train_size - val_size\n","\n","train_set, val_set, test_set = random_split(\n","    dataset,\n","    [train_size, val_size, test_size],\n","    generator=torch.Generator().manual_seed(42)\n",")\n","\n","print(f\"Completed split: {len(train_set)} training set, {len(val_set)} validation set, {len(test_set)} test set\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZER_ioRZnfy","executionInfo":{"status":"ok","timestamp":1772100167681,"user_tz":-60,"elapsed":55,"user":{"displayName":"Matteo Gerevini","userId":"01604553129882149651"}},"outputId":"28a31d5e-b578-40b5-937f-2ab245f0391e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Completed split: 2518 training set, 540 validation set, 540 test set\n"]}]},{"cell_type":"markdown","source":["# **Model 2: Custom Convolutional Autoencoder Classifier**"],"metadata":{"id":"g_I4r9_1b5ZG"}},{"cell_type":"markdown","source":["## **Data Preprocessing & DataLoader**"],"metadata":{"id":"ot0UxESNgT6s"}},{"cell_type":"markdown","source":["In this section, we set up the data transformations and the DataLoader instances. Since the main goal of our Autoencoder is to achieve a highly accurate reconstruction of the original images, unlike the ResNet-18 classifier, we had to take a completely different approach for the preprocessing pipeline:\n","\n","* We decided to skip data augmentation entirely. This means we purposely left out transformations like `RandomCrop()`, `RandomRotation()`, and `RandomHorizontalFlip()` from our training set. The reason is simple: the autoencoder needs to learn the actual spatial representation of the leather textures and their defects, without having to struggle with reconstructing pixels that have been artificially shifted or distorted.\n","* For resizing, we went with a more direct approach. Instead of doing a `Resize(230)` followed by a `RandomCrop(224)`, which we previously used to introduce translation invariance, we just applied a straightforward `Resize((224, 224))` using standard bilinear interpolation. This lets us smoothly compress the original $227 \\times 227$ images without throwing away any important information around the borders, where some defects might actually be located.\n","* Finally, we changed how we handle pixel scaling by avoiding zero-centered normalization. In the ResNet-18 model, we mapped pixels to a [-1.0, 1.0] range to help with gradient flow during classification. Here, though, we only rely on `ToTensor()`, which keeps the pixel values in the [0.0, 1.0] range. This was a strict architectural requirement because the final layer of our decoder uses a Sigmoid activation function, which naturally outputs values between 0 and 1. By making sure the input and output domains match perfectly, our Mean Squared Error (MSE) loss can compute the reconstruction error accurately."],"metadata":{"id":"Z6fOhi8KhaPz"}},{"cell_type":"code","source":["from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","class TransformSubset(Dataset):\n","    def __init__(self, subset, transform=None):\n","        self.subset = subset\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        x, y = self.subset[index]\n","        if self.transform:\n","            x = self.transform(x)\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.subset)\n","\n","ae_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])\n","\n","train_final = TransformSubset(train_set, transform=ae_transform)\n","val_final = TransformSubset(val_set, transform=ae_transform)\n","test_final = TransformSubset(test_set, transform=ae_transform)\n","\n","batch_size = 32\n","\n","train_loader = DataLoader(train_final, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_final, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_final, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"MXENr5Pxb50t","executionInfo":{"status":"ok","timestamp":1772102913014,"user_tz":-60,"elapsed":11,"user":{"displayName":"Matteo Gerevini","userId":"01604553129882149651"}}},"execution_count":8,"outputs":[]}]}